{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_span_params = {'adapt_span_enabled': True, 'attn_span': 1024, 'adapt_span_loss': 0, 'adapt_span_ramp': 32, 'adapt_span_init': 0,\n",
    "                     'adapt_span_cache': False, 'nb_heads': 12, 'bs':128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attention_heads = 12\n",
    "attention_head_size = 768//12\n",
    "all_head_size = 12*64\n",
    "hidden_size = 768\n",
    "\n",
    "query = nn.Linear(hidden_size, all_head_size) # 768,768\n",
    "key = nn.Linear(hidden_size, all_head_size)\n",
    "value = nn.Linear(hidden_size, all_head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.rand(128,36,768)\n",
    "context = torch.rand(128,20,768)\n",
    "attention_mask = torch.rand(128,1,1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 36, 768])\n",
      "torch.Size([128, 20, 768])\n",
      "torch.Size([128, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "q = query(hidden_states)\n",
    "print(q.shape)\n",
    "k = key(context)\n",
    "print(k.shape)\n",
    "v = key(context)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _skew(X, pad_value):\n",
    "    \"\"\"shift every row 1 step to right\"\"\"\n",
    "    # X = B x M x L\n",
    "    B, M, L = X.size()\n",
    "    X = F.pad(X, (0, M + 1), value=pad_value)  # B x M x (L+M+1)\n",
    "    X = X.view(B, -1)  # B x ML+MM+M\n",
    "    X = X[:, :-M]  # B x ML+MM\n",
    "    X = X.view(B, M, M + L)  # B x M x L+M\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unskew(X):\n",
    "    \"\"\"reverse _skew operation\"\"\"\n",
    "    # X = B x M x L+M\n",
    "    B, M, L = X.size()\n",
    "    L -= M\n",
    "    X = X.view(B, -1)  # B x ML+MM\n",
    "    X = F.pad(X, (0, M))  # B x ML+MM+M\n",
    "    X = X.view(B, M, M + L + 1)  # B x M x L+M+1\n",
    "    X = X[:, :, :L]  # B x M x L\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = adapt_span_params['attn_span']    # [attn_span]\n",
    "ramp_size = adapt_span_params['adapt_span_ramp']\n",
    "shape = (128,adapt_span_params['bs'], 1,1)\n",
    "init_val = adapt_span_params['adapt_span_init']\n",
    "current_val = nn.init.uniform_(nn.Parameter(torch.zeros(*shape) + init_val)) # [bs,nb_heads,1,1]\n",
    "mask_template = torch.linspace(1 - max_size, 0, steps=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1060, 768])\n",
      "torch.Size([128, 1060, 768])\n",
      "torch.Size([1, 64, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptive_span = AdaptiveSpan(**adapt_span_params)\n",
    "attn_span = adapt_span_params['attn_span']\n",
    "k_pe = nn.Parameter(torch.randn(1, hidden_size // num_attention_heads, attn_span))\n",
    "\n",
    "k, v, k_pe = adaptive_span.trim_memory(q,k,v,k_pe)\n",
    "print(k.shape),print(v.shape), print(k_pe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 36, 64]) torch.Size([128, 12, 1060, 64]) torch.Size([128, 12, 1060, 64])\n"
     ]
    }
   ],
   "source": [
    "q = transpose_for_scores(q) \n",
    "k = transpose_for_scores(k)\n",
    "v = transpose_for_scores(v) \n",
    "print(q.shape, k.shape, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 12, 64, 1060])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-1,-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 36, 1060])\n"
     ]
    }
   ],
   "source": [
    "attention_cont = torch.matmul(q, k.transpose(-1, -2))\n",
    "print(attention_cont.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0,d1,d2,d3 = attention_cont.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_cont = torch.reshape(attention_cont, (d0*d1,d2,d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1536, 36, 1024])\n"
     ]
    }
   ],
   "source": [
    "attention_cont = _unskew(attention_cont)\n",
    "print(attention_cont.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 36, 1024])\n"
     ]
    }
   ],
   "source": [
    "attention_cont = torch.reshape(attention_cont, (d0,d1,d2,-1))\n",
    "print(attention_cont.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 12, 36, 1024])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_pos = torch.matmul(q, k_pe)\n",
    "attention_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = attention_cont+attention_pos\n",
    "attention_scores = attention_scores/math.sqrt(hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = nn.Linear(20,1024)(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = attention_scores + attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 12, 36, 1024])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_probs = nn.Softmax(dim=-1)(attention_scores) # [128, 12, 20, 52]\n",
    "#print(attention_probs.shape)\n",
    "attention_probs = adaptive_span(attention_probs)\n",
    "attention_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
