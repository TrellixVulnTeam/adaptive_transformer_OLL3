{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0113 23:23:36.288935 139673391356800 file_utils.py:35] PyTorch version 1.3.1+cpu available.\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_bert import BertLayerNorm\n",
    "from transformers import BertTokenizer\n",
    "from lxrt.modeling import BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _skew(X, pad_value):\n",
    "    \"\"\"shift every row 1 step to right\"\"\"\n",
    "    # X = B x M x L\n",
    "    B, M, L = X.size()\n",
    "    X = F.pad(X, (0, M + 1), value=pad_value)  # B x M x (L+M+1)\n",
    "    X = X.view(B, -1)  # B x ML+MM+M\n",
    "    X = X[:, :-M]  # B x ML+MM\n",
    "    X = X.view(B, M, M + L)  # B x M x L+M\n",
    "    return X\n",
    "\n",
    "\n",
    "def _unskew(X):\n",
    "    \"\"\"reverse _skew operation\"\"\"\n",
    "    # X = B x M x L+M\n",
    "    B, M, L = X.size()\n",
    "    L -= M\n",
    "    X = X.view(B, -1)  # B x ML+MM\n",
    "    X = F.pad(X, (0, M))  # B x ML+MM+M\n",
    "    X = X.view(B, M, M + L + 1)  # B x M x L+M+1\n",
    "    X = X[:, :, :L]  # B x M x L\n",
    "    return X\n",
    "\n",
    "class AdaptiveMask(nn.Module):\n",
    "    \"\"\"Soft masking function for adaptive size.\n",
    "    It masks out the last K values of an input. The masking value\n",
    "    goes from 1 to 0 gradually, so K can be learned with\n",
    "    back-propagation.\n",
    "    Args:\n",
    "        max_size: maximum size (i.e. input dimension)\n",
    "        ramp_size: size of the ramp going from 0 to 1\n",
    "        init_val: initial size proportion not to be masked out\n",
    "        shape: learn multiple sizes independent of each other\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size, ramp_size, init_val=0, shape=(1,)):\n",
    "        nn.Module.__init__(self)\n",
    "        self._max_size = max_size                                       # 32\n",
    "        self._ramp_size = ramp_size\n",
    "        self.current_val = nn.Parameter(torch.zeros(*shape) + init_val) # [12,1,1]\n",
    "        mask_template = torch.linspace(1 - max_size, 0, steps=max_size) # 32\n",
    "        self.register_buffer('mask_template', mask_template)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.mask_template + self.current_val * self._max_size # [12,1,32]\n",
    "        mask = mask / self._ramp_size + 1                             # [12,1,32]\n",
    "        mask = mask.clamp(0, 1)\n",
    "        if x.size(-1) < self._max_size:\n",
    "            # the input could have been trimmed beforehand to save computation\n",
    "            mask = mask[:, :, -x.size(-1):]\n",
    "        #print(x.shape, mask.shape)\n",
    "        x = x * mask # [128, 12, 20, 32], [12, 1, 32]\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "    def get_current_max_size(self, include_ramp=True):\n",
    "        current_size = math.ceil(self.current_val.max().item() * self._max_size)\n",
    "        if include_ramp:\n",
    "            current_size += self._ramp_size\n",
    "        current_size = max(0, min(self._max_size, current_size))\n",
    "        return current_size\n",
    "\n",
    "    def get_current_avg_size(self, include_ramp=True):\n",
    "        current_size = math.ceil(self.current_val.mean().item() * self._max_size)\n",
    "        if include_ramp:\n",
    "            current_size += self._ramp_size\n",
    "        current_size = max(0, min(self._max_size, current_size))\n",
    "        return current_size\n",
    "\n",
    "    def clamp_param(self):\n",
    "        \"\"\"this need to be called after each update\"\"\"\n",
    "        self.current_val.data.clamp_(0, 1)\n",
    "\n",
    "\n",
    "class AdaptiveSpan(nn.Module):\n",
    "    \"\"\"Adaptive attention span for Transformerself.\n",
    "    This module learns an attention span length from data for each\n",
    "    self-attention head.\n",
    "    Args:\n",
    "        attn_span: maximum attention span\n",
    "        adapt_span_loss: loss coefficient for the span length\n",
    "        adapt_span_ramp: length of the masking ramp\n",
    "        adapt_span_init: initial size ratio\n",
    "        adapt_span_cache: adapt cache size to reduce memory usage\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_span, adapt_span_loss, adapt_span_ramp,\n",
    "                 adapt_span_init, adapt_span_cache, nb_heads, **kargs):\n",
    "        nn.Module.__init__(self)\n",
    "        self._adapt_cache = adapt_span_cache\n",
    "        self._max_span = attn_span\n",
    "        self._loss_coeff = adapt_span_loss\n",
    "        self._nb_heads = nb_heads\n",
    "        self._mask = AdaptiveMask(max_size=self._max_span,\n",
    "                                 ramp_size=adapt_span_ramp,\n",
    "                                 init_val=adapt_span_init,\n",
    "                                 shape=(128,nb_heads,1, 1)) \n",
    "        # TO-DO: batch_size to be dynamically controlled\n",
    "        \n",
    "        self.attn_linear = nn.Linear(attn_span+20,attn_span)\n",
    "        \n",
    "    def forward(self, attn):\n",
    "        \"\"\"mask attention with the right span\"\"\"\n",
    "        # batch and head dimensions are merged together, so separate them first\n",
    "        B = attn.size(0) # batch size\n",
    "        M = attn.size(1) # block size\n",
    "        #attn = attn.reshape(B // self._nb_heads, self._nb_heads, M, -1)\n",
    "        #################   Project into same embedding space ##################\n",
    "        attn = self.attn_linear(attn)\n",
    "        ########################################################################\n",
    "        attn = self._mask(attn)\n",
    "        attn = attn / (attn.sum(-1, keepdim=True) + 1e-8)  # normalize so sum is 1\n",
    "\n",
    "        #attn = attn.view(B, M, -1)\n",
    "        return attn\n",
    "\n",
    "    def get_trim_len(self):\n",
    "        \"\"\"how much of memory can be trimmed to reduce computation\"\"\"\n",
    "        L = self._max_span\n",
    "        trim_len = min(L - 1, L - self._mask.get_current_max_size())\n",
    "        # too fine granularity might be bad for the memory management\n",
    "        trim_len = math.floor(trim_len / 64) * 64\n",
    "        return trim_len\n",
    "\n",
    "    def trim_memory(self, query, key, value, key_pe):\n",
    "        \"\"\"trim out unnecessary memory beforehand to reduce computation\"\"\"\n",
    "        trim_len = self.get_trim_len()\n",
    "        cache_size = key.size(1) - query.size(1)\n",
    "        trim_len_cache = trim_len - (self._max_span - cache_size)\n",
    "        if trim_len_cache > 0:\n",
    "            key = key[:, trim_len_cache:, :]\n",
    "            value = value[:, trim_len_cache:, :]\n",
    "        elif trim_len_cache < 0:\n",
    "            # cache is too short! this happens when validation resumes\n",
    "            # after a lot of updates.\n",
    "            key = F.pad(key, [0, 0, -trim_len_cache, 0])\n",
    "            value = F.pad(value, [0, 0, -trim_len_cache, 0])\n",
    "        if trim_len > 0:\n",
    "            if key_pe is not None:\n",
    "                key_pe = key_pe[:, :, trim_len:]\n",
    "        return key, value, key_pe\n",
    "\n",
    "    def get_cache_size(self):\n",
    "        \"\"\"determine how long the cache should be\"\"\"\n",
    "        if self._adapt_cache:\n",
    "            trim_len = self.get_trim_len()\n",
    "            # give a buffer of 64 steps since a span might increase\n",
    "            # in future updates\n",
    "            return min(self._max_span, self._max_span - trim_len + 64)\n",
    "        else:\n",
    "            return self._max_span\n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\"a loss term for regularizing the span length\"\"\"\n",
    "        return self._loss_coeff * self._max_span * self._mask.current_val.mean()\n",
    "\n",
    "    def get_current_max_span(self):\n",
    "        return self._mask.get_current_max_size()\n",
    "\n",
    "    def get_current_avg_span(self):\n",
    "        return self._mask.get_current_avg_size()\n",
    "\n",
    "    def clamp_param(self):\n",
    "        self._mask.clamp_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_span_params = {'adapt_span_enabled': True, 'attn_span': 32, 'adapt_span_loss': 0, 'adapt_span_ramp': 32, 'adapt_span_init': 0,\n",
    "                     'adapt_span_cache': False, 'nb_heads': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 20, 32]) torch.Size([128, 12, 1, 32])\n",
      "torch.Size([128, 12, 20, 32])\n"
     ]
    }
   ],
   "source": [
    "adaptive_span = AdaptiveSpan(**adapt_span_params)\n",
    "query, key, value = torch.rand(128,20,768), torch.rand(128,36,768), torch.rand(128,36,768)\n",
    "key_layer, value_layer, key_pe = adaptive_span.trim_memory(query,key,value,nn.Parameter(\n",
    "            torch.randn(1, 768 // 12, 32)))\n",
    "key_layer.shape, value_layer.shape, key_pe.shape\n",
    "att = adaptive_span(torch.rand(128,12,20,52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxrt.entry import InputFeatures,convert_sents_to_features,set_visual_config\n",
    "from lxrt.modeling import VISUAL_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "bert_config = BertConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return F.gelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self,l_layers,x_layers,r_layers):\n",
    "        self.llayers = l_layers\n",
    "        self.xlayers = x_layers\n",
    "        self.rlayers = r_layers\n",
    "        self.from_scratch=False\n",
    "args = Args(9,5,5)\n",
    "MAX_VQA_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BertEmbeddings\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=0)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size, padding_idx=0)\n",
    "\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "    \n",
    "from transformers import BertConfig\n",
    "bert_embeddings = BertEmbeddings(BertConfig())\n",
    "output = bert_embeddings(input_ids = torch.rand(128,20).long(),\n",
    "                         token_type_ids = torch.rand(128,20).long())\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BertAttention\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    from transformers import BertConfig\n",
    "    \n",
    "    bert_att = BertAttention(BertConfig())\n",
    "    context_output = bert_att(hidden_states = torch.rand(128,20,768),\n",
    "                          context = torch.rand(128,36,768),\n",
    "                          attention_mask = None)\n",
    "    context_output.shape # [128, 20, 768]\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config, ctx_dim=None, adapt_span_params=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
    "        self.num_attention_heads = config.num_attention_heads # 12\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads) # 768/12\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size # 12*64\n",
    "\n",
    "        # visual_dim = 2048\n",
    "        if ctx_dim is None:\n",
    "            ctx_dim =config.hidden_size\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size) # 768x768\n",
    "        self.key = nn.Linear(ctx_dim, self.all_head_size) # 768x768\n",
    "        self.value = nn.Linear(ctx_dim, self.all_head_size) # 768x768\n",
    "        \n",
    "        attn_span = adapt_span_params['attn_span']\n",
    "        \n",
    "        self.key_pe = nn.Parameter(\n",
    "            torch.randn(1, config.hidden_size // self.num_attention_heads, attn_span))\n",
    "        \n",
    "        self.key_pe_lin = nn.Linear(attn_span,attn_span+20)\n",
    "        self.val_layer_project = nn.Linear(attn_span+20,attn_span)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        \n",
    "        self.adapt_span_enabled = adapt_span_params['adapt_span_enabled']\n",
    "        \n",
    "        if self.adapt_span_enabled:\n",
    "            self.adaptive_span = AdaptiveSpan(**adapt_span_params)\n",
    "        \n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, context, attention_mask=None):\n",
    "        \n",
    "        mixed_query_layer = self.query(hidden_states) # [128,20,768]\n",
    "        mixed_key_layer = self.key(context) # [128,36,768]\n",
    "        mixed_value_layer = self.value(context) #[128,36,768]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.adapt_span_enabled:\n",
    "            mixed_key_layer, mixed_value_layer, key_pe = self.adaptive_span.trim_memory(mixed_query_layer,\n",
    "                                                                            mixed_key_layer,\n",
    "                                                                            mixed_value_layer,\n",
    "                                                                            self.key_pe)\n",
    "            \n",
    "        # mixed_key_layer -> [128, 52, 768],\n",
    "        # mixed_value_layer -> [128, 52, 768]\n",
    "        # key_pe -> [1, 64, 32])\n",
    "    \n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer) # [128, 12, 20, 64]\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer) # [128, 12, 52, 64]\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer) # [128, 12, 52, 64]\n",
    "        \n",
    "        \n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        # print('key_layer_transpose: ', key_layer.transpose(-1,-2).shape) : [128, 12, 64, 36]\n",
    "    \n",
    "        attention_cont = torch.matmul(query_layer, key_layer.transpose(-1, -2)) # [128, 12, 20, 52]\n",
    "        \n",
    "        #attention_cont = _unskew(attention_cont)\n",
    "        \n",
    "        attention_pos = torch.matmul(query_layer, key_pe) # [128,12,20,32]\n",
    "        \n",
    "############     Project into same embedding space ###############################     \n",
    "        attention_pos = self.key_pe_lin(attention_pos) # [128,12,20,52]\n",
    "###################################################################################    \n",
    "\n",
    "        attention_scores = attention_cont + attention_pos\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size) # [128, 12, 20, 52]\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores) # [128, 12, 20, 52]\n",
    "        #print(attention_probs.shape)\n",
    "        if self.adapt_span_enabled:\n",
    "            attention_probs = self.adaptive_span(attention_probs) # [128,12,640]\n",
    "        \n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        \n",
    "        attention_probs = self.dropout(attention_probs) # [128, 12, 20, 36]\n",
    "        \n",
    "        #attention_probs = _skew(attention_probs,0)\n",
    "        value_layer = value_layer.permute(0,1,3,2) # [128,12,64,52]\n",
    "        value_layer = self.val_layer_project(value_layer).permute(0,1,3,2) # [128,12,32,64]\n",
    "        \n",
    "        #  Earlier: [128, 12, 20, 32] x [128, 12, 32, 64]\n",
    "        print(attention_probs.shape, value_layer.shape)\n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, value_layer) # [128, 12, 20, 64]\n",
    "        \n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous() # [128, 20, 12, 64]\n",
    "        \n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,) # [128, 20, 768]\n",
    "        \n",
    "        context_layer = context_layer.view(*new_context_layer_shape) # [128, 20, 768]\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 20, 32]) torch.Size([128, 12, 32, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20, 768])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "bert_att = BertAttention(BertConfig(),adapt_span_params=adapt_span_params)\n",
    "context_output = bert_att(hidden_states = torch.rand(128,20,768),\n",
    "                          context = torch.rand(128,36,768),\n",
    "                          attention_mask = None)\n",
    "context_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20, 768])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BertAttOutput\n",
    "\n",
    "class BertAttOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    from transformers import BertConfig\n",
    "    bert_att_output = BertAttOutput(BertConfig())\n",
    "    output = bert_att_output(torch.rand(128,20,768),torch.rand(128,20,768))\n",
    "    output.shape [128,20,768]\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertAttOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "from transformers import BertConfig\n",
    "bert_att_output = BertAttOutput(BertConfig())\n",
    "output = bert_att_output(torch.rand(128,20,768),torch.rand(128,20,768))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 20, 32]) torch.Size([128, 12, 32, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20, 768])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BertCross Attention\n",
    "class BertCrossattLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    from transformers import BertConfig\n",
    "    \n",
    "    bert_cross_att = BertCrossattLayer(BertConfig())\n",
    "    output = bert_cross_att(input_tensor = torch.rand(128,20,768), \n",
    "                        ctx_tensor = torch.rand(128,36,768), \n",
    "                        ctx_att_mask = None)\n",
    "                        \n",
    "    output.shape [128,20,768]\n",
    "    \"\"\"\n",
    "    def __init__(self, config,adapt_span_params):\n",
    "        super().__init__()\n",
    "        self.att = BertAttention(config,adapt_span_params=adapt_span_params)\n",
    "        self.output = BertAttOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, ctx_tensor, ctx_att_mask=None):\n",
    "        output = self.att(input_tensor, ctx_tensor, ctx_att_mask) # [128,20,768]\n",
    "        attention_output = self.output(output, input_tensor)\n",
    "        return attention_output\n",
    "    \n",
    "from transformers import BertConfig\n",
    "bert_cross_att = BertCrossattLayer(BertConfig(),adapt_span_params=adapt_span_params)\n",
    "output = bert_cross_att(input_tensor = torch.rand(128,20,768), \n",
    "                        ctx_tensor = torch.rand(128,36,768), \n",
    "                        ctx_att_mask = None)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 12, 20, 32]) torch.Size([128, 12, 32, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20, 768])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BertSelfattLayer\n",
    "\n",
    "class BertSelfattLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    bert_self_att_layer = BertSelfattLayer(bert_config)\n",
    "    output = bert_self_att_layer(input_tensor = torch.rand(128,20,768),\n",
    "                             attention_mask = torch.rand(128,1,1,20))\n",
    "    output.shape [128, 20, 768]\n",
    "    \"\"\"\n",
    "    def __init__(self, config,adapt_span_params=adapt_span_params):\n",
    "        super(BertSelfattLayer, self).__init__()\n",
    "        self.self = BertAttention(config,adapt_span_params=adapt_span_params)\n",
    "        self.output = BertAttOutput(config)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        # Self attention attends to itself, thus keys and querys are the same (input_tensor).\n",
    "        self_output = self.self(input_tensor, input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output\n",
    "    \n",
    "bert_self_att_layer = BertSelfattLayer(bert_config,adapt_span_params=adapt_span_params)\n",
    "output = bert_self_att_layer(input_tensor = torch.rand(128,20,768),\n",
    "                             attention_mask = torch.rand(128,1,1,52))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
