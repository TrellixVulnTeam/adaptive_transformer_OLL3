{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0109 21:30:20.533334 140713261415296 file_utils.py:35] PyTorch version 1.3.1+cpu available.\n",
      "/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from utils import load_obj_tsv\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "# Load part of the dataset for fast checking.\n",
    "# Notice that here is the number of images instead of the number of data,\n",
    "# which means all related data to the images would be used.\n",
    "TINY_IMG_NUM = 512\n",
    "FAST_IMG_NUM = 5000\n",
    "\n",
    "# The path to data and image features.\n",
    "\n",
    "MSCOCO_IMGFEAT_ROOT = '/home/u37216/project/lxmert/data/mscoco_img_feat/'\n",
    "#'/home/jupyter/vcr/lxmert/data/mscoco_imgfeat/'\n",
    "SPLIT2NAME = {\n",
    "    'train': 'train2014',\n",
    "    'valid': 'val2014',\n",
    "    'minival': 'val2014',\n",
    "    'nominival': 'val2014',\n",
    "    'test': 'test2015',\n",
    "}\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from param import args;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train='train'\n",
    "valid='valid'\n",
    "test=None\n",
    "bs=128\n",
    "optim=torch.optim.Adam\n",
    "lr=1e-4\n",
    "epochs=10\n",
    "dpt=0.1\n",
    "seed=9595\n",
    "output_dir='test'\n",
    "fast=False\n",
    "tiny=False\n",
    "tqdm=True\n",
    "load=None\n",
    "load_lxmert=None\n",
    "load_lxmert_qa=None\n",
    "from_scratch=False\n",
    "mce_loss=False\n",
    "llayers=9\n",
    "xlayers = 5\n",
    "rlayers=5\n",
    "taskMatched = False\n",
    "taskMaskLM=False\n",
    "taskObjPredict=False\n",
    "taskQA=False\n",
    "visualLosses='obj,attr,feat'\n",
    "qaSets=None\n",
    "wordMaskRate=0.15 \n",
    "objMaskRate=0.15\n",
    "multiGPU=False\n",
    "num_workers=4  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base class for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset:\n",
    "    \"\"\"\n",
    "    A VQA data example in json file:\n",
    "        {\n",
    "            \"answer_type\": \"other\",\n",
    "            \"img_id\": \"COCO_train2014_000000458752\",\n",
    "            \"label\": {\n",
    "                \"net\": 1\n",
    "            },\n",
    "            \"question_id\": 458752000,\n",
    "            \"question_type\": \"what is this\",\n",
    "            \"sent\": \"What is this photo taken looking through?\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self, path,splits: str):\n",
    "        self.path = path\n",
    "        self.name = splits\n",
    "        self.splits = splits.split(',')\n",
    "\n",
    "        # Loading datasets\n",
    "        self.data = []\n",
    "        for split in self.splits:\n",
    "            self.data.extend(json.load(open(self.path+\"%s.json\" % split)))\n",
    "        print(\"Load %d data from split(s) %s.\" % (len(self.data), self.name))\n",
    "\n",
    "        # Convert list to dict (for evaluation)\n",
    "        self.id2datum = {\n",
    "            datum['question_id']: datum\n",
    "            for datum in self.data\n",
    "        }\n",
    "\n",
    "        # Answers\n",
    "        self.ans2label = json.load(open(self.path+\"trainval_ans2label.json\"))\n",
    "        self.label2ans = json.load(open(self.path+\"trainval_label2ans.json\"))\n",
    "        assert len(self.ans2label) == len(self.label2ans)\n",
    "\n",
    "    @property\n",
    "    def num_answers(self):\n",
    "        return len(self.ans2label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def plot_img(self,idx):\n",
    "        im = cv.imread(self.path+self.splits[0]+'2014'+'/'+ self.data[idx]['img_id']+'.jpg')\n",
    "        im = plt.imshow(im)\n",
    "        return im,self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dset_train_init.plot_img(125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is\n",
    "# -ans2label, label2ans from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dset_train_init.data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQATorchDataset(Dataset):\n",
    "    def __init__(self, dataset: VQADataset):\n",
    "        super().__init__()\n",
    "        self.raw_dataset = dataset\n",
    "\n",
    "        if tiny:\n",
    "            topk = TINY_IMG_NUM\n",
    "        elif fast:\n",
    "            topk = FAST_IMG_NUM\n",
    "        else:\n",
    "            topk = None\n",
    "\n",
    "        # Loading detection features to img_data\n",
    "        img_data = []\n",
    "        for split in dataset.splits:\n",
    "            # Minival is 5K images in MS COCO, which is used in evaluating VQA/LXMERT-pre-training.\n",
    "            # It is saved as the top 5K features in val2014_***.tsv\n",
    "            load_topk = 5000 if (split == 'minival' and topk is None) else topk\n",
    "            img_data.extend(load_obj_tsv(\n",
    "                os.path.join(MSCOCO_IMGFEAT_ROOT, '%s_obj36.tsv' % (SPLIT2NAME[split])),\n",
    "                topk=load_topk))\n",
    "\n",
    "        # Convert img list to dict\n",
    "        self.imgid2img = {}\n",
    "        for img_datum in img_data:\n",
    "            self.imgid2img[img_datum['img_id']] = img_datum\n",
    "\n",
    "        # Only kept the data with loaded image features\n",
    "        self.data = []\n",
    "        for datum in self.raw_dataset.data:\n",
    "            if datum['img_id'] in self.imgid2img:\n",
    "                self.data.append(datum)\n",
    "        print(\"Use %d data in torch dataset\" % (len(self.data)))\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item: int):\n",
    "        datum = self.data[item]\n",
    "\n",
    "        img_id = datum['img_id']\n",
    "        ques_id = datum['question_id']\n",
    "        ques = datum['sent']\n",
    "\n",
    "        # Get image info\n",
    "        img_info = self.imgid2img[img_id]\n",
    "        obj_num = img_info['num_boxes']\n",
    "        feats = img_info['features'].copy()\n",
    "        boxes = img_info['boxes'].copy()\n",
    "        assert obj_num == len(boxes) == len(feats)\n",
    "\n",
    "        # Normalize the boxes (to 0 ~ 1)\n",
    "        img_h, img_w = img_info['img_h'], img_info['img_w']\n",
    "        boxes = boxes.copy()\n",
    "        boxes[:, (0, 2)] /= img_w\n",
    "        boxes[:, (1, 3)] /= img_h\n",
    "        np.testing.assert_array_less(boxes, 1+1e-5)\n",
    "        np.testing.assert_array_less(-boxes, 0+1e-5)\n",
    "\n",
    "        # Provide label (target)\n",
    "        if 'label' in datum:\n",
    "            label = datum['label']\n",
    "            target = torch.zeros(self.raw_dataset.num_answers)\n",
    "            for ans, score in label.items():\n",
    "                target[self.raw_dataset.ans2label[ans]] = score\n",
    "            return ques_id, feats, boxes, ques, target\n",
    "        else:\n",
    "            return ques_id, feats, boxes, ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAEvaluator:\n",
    "    def __init__(self, dataset: VQADataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def evaluate(self, quesid2ans: dict):\n",
    "        score = 0.\n",
    "        for quesid, ans in quesid2ans.items():\n",
    "            datum = self.dataset.id2datum[quesid]\n",
    "            label = datum['label']\n",
    "            if ans in label:\n",
    "                score += label[ans]\n",
    "        return score / len(quesid2ans)\n",
    "\n",
    "    def dump_result(self, quesid2ans: dict, path):\n",
    "        \"\"\"\n",
    "        Dump results to a json file, which could be submitted to the VQA online evaluation.\n",
    "        VQA json file submission requirement:\n",
    "            results = [result]\n",
    "            result = {\n",
    "                \"question_id\": int,\n",
    "                \"answer\": str\n",
    "            }\n",
    "        :param quesid2ans: dict of quesid --> ans\n",
    "        :param path: The desired path of saved file.\n",
    "        \"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            result = []\n",
    "            for ques_id, ans in quesid2ans.items():\n",
    "                result.append({\n",
    "                    'question_id': ques_id,\n",
    "                    'answer': ans\n",
    "                })\n",
    "            json.dump(result, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 25994 data from split(s) minival.\n",
      "Load 25994 data from split(s) minival.\n"
     ]
    }
   ],
   "source": [
    "VQA_DATA_ROOT = '/home/jupyter/vcr/lxmert/data/vqa/'\n",
    "dset_train_init = VQADataset(VQA_DATA_ROOT,'minival') #'train,nominival'\n",
    "dset_valid_init = VQADataset(VQA_DATA_ROOT,'minival')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load Faster-RCNN detected objects from /home/jupyter/vcr/lxmert/data/mscoco_imgfeat/val2014_obj36.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4993it [00:25, 198.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 images in file /home/jupyter/vcr/lxmert/data/mscoco_imgfeat/val2014_obj36.tsv in 25 seconds.\n",
      "Use 25994 data in torch dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = VQATorchDataset(dset_train_init) # ques_id, feats, boxes, ques, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dset = VQAEvaluator(dset_valid_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LXMERT QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain.qa_answer_table import load_lxmert_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxrt.entry import LXRTEncoder\n",
    "from torch.nn.functional import gelu\n",
    "from transformers.modeling_bert import BertLayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length including <bos> and <eos>\n",
    "MAX_VQA_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        return torch.nn.functional.gelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self,l_layers,x_layers,r_layers):\n",
    "        self.llayers = l_layers\n",
    "        self.xlayers = x_layers\n",
    "        self.rlayers = r_layers\n",
    "        self.from_scratch=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(9,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build LXRT encoder\n",
    "        self.lxrt_encoder = LXRTEncoder(\n",
    "            args,\n",
    "            max_seq_length=MAX_VQA_LENGTH\n",
    "        )\n",
    "        hid_dim = self.lxrt_encoder.dim\n",
    "        \n",
    "        # VQA Answer heads\n",
    "        self.logit_fc = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim * 2),\n",
    "            GeLU(),\n",
    "            BertLayerNorm(hid_dim * 2, eps=1e-12),\n",
    "            nn.Linear(hid_dim * 2, num_answers)\n",
    "        )\n",
    "        self.logit_fc.apply(self.lxrt_encoder.model.init_bert_weights)\n",
    "\n",
    "    def forward(self, feat, pos, sent):\n",
    "        \"\"\"\n",
    "        b -- batch_size, o -- object_number, f -- visual_feature_size\n",
    "        :param feat: (b, o, f)\n",
    "        :param pos:  (b, o, 4)\n",
    "        :param sent: (b,) Type -- list of string\n",
    "        :param leng: (b,) Type -- int numpy array\n",
    "        :return: (b, num_answer) The logit of each answers.\n",
    "        \"\"\"\n",
    "        x = self.lxrt_encoder(sent, (feat, pos))\n",
    "        logit = self.logit_fc(x)\n",
    "\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_tuple(splits: str, bs:int, shuffle=False, drop_last=False) -> DataTuple:\n",
    "#     dset = VQADataset(splits)\n",
    "#     tset = VQATorchDataset(dset)\n",
    "#     evaluator = VQAEvaluator(dset)\n",
    "#     data_loader = DataLoader(\n",
    "#         tset, batch_size=bs,\n",
    "#         shuffle=shuffle, num_workers=args.num_workers,\n",
    "#         drop_last=drop_last, pin_memory=True\n",
    "#     )\n",
    "\n",
    "#     return DataTuple(dataset=dset, loader=data_loader, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "        train_data, batch_size=bs,\n",
    "        shuffle=True, num_workers=num_workers,\n",
    "        drop_last=False, pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4993it [00:40, 198.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n"
     ]
    }
   ],
   "source": [
    "model = VQAModel(dset_train_init.num_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_lxmert_qa_path = '/home/jupyter/vcr/lxmert/snap/pretrained/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerTable:\n",
    "    ANS_CONVERT = {\n",
    "        \"a man\": \"man\",\n",
    "        \"the man\": \"man\",\n",
    "        \"a woman\": \"woman\",\n",
    "        \"the woman\": \"woman\",\n",
    "        'one': '1',\n",
    "        'two': '2',\n",
    "        'three': '3',\n",
    "        'four': '4',\n",
    "        'five': '5',\n",
    "        'six': '6',\n",
    "        'seven': '7',\n",
    "        'eight': '8',\n",
    "        'nine': '9',\n",
    "        'ten': '10',\n",
    "        'grey': 'gray',\n",
    "    }\n",
    "\n",
    "    def __init__(self, dsets=None):\n",
    "        self.all_ans = json.load(open(\"/home/jupyter/vcr/lxmert/data/lxmert/all_ans.json\"))\n",
    "        if dsets is not None:\n",
    "            dsets = set(dsets)\n",
    "            # If the answer is used in the dsets\n",
    "            self.anss = [ans['ans'] for ans in self.all_ans if\n",
    "                         len(set(ans['dsets']) & dsets) > 0]\n",
    "        else:\n",
    "            self.anss = [ans['ans'] for ans in self.all_ans]\n",
    "        self.ans_set = set(self.anss)\n",
    "\n",
    "        self._id2ans_map = self.anss\n",
    "        self._ans2id_map = {ans: ans_id for ans_id, ans in enumerate(self.anss)}\n",
    "\n",
    "        assert len(self._id2ans_map) == len(self._ans2id_map)\n",
    "        for ans_id, ans in enumerate(self._id2ans_map):\n",
    "            assert self._ans2id_map[ans] == ans_id\n",
    "\n",
    "    def convert_ans(self, ans):\n",
    "        if len(ans) == 0:\n",
    "            return \"\"\n",
    "        ans = ans.lower()\n",
    "        if ans[-1] == '.':\n",
    "            ans = ans[:-1].strip()\n",
    "        if ans.startswith(\"a \"):\n",
    "            ans = ans[2:].strip()\n",
    "        if ans.startswith(\"an \"):\n",
    "            ans = ans[3:].strip()\n",
    "        if ans.startswith(\"the \"):\n",
    "            ans = ans[4:].strip()\n",
    "        if ans in self.ANS_CONVERT:\n",
    "            ans = self.ANS_CONVERT[ans]\n",
    "        return ans\n",
    "\n",
    "    def ans2id(self, ans):\n",
    "        return self._ans2id_map[ans]\n",
    "\n",
    "    def id2ans(self, ans_id):\n",
    "        return self._id2ans_map[ans_id]\n",
    "\n",
    "    def ans2id_map(self):\n",
    "        return self._ans2id_map.copy()\n",
    "\n",
    "    def id2ans_map(self):\n",
    "        return self._id2ans_map.copy()\n",
    "\n",
    "    def used(self, ans):\n",
    "        return ans in self.ans_set\n",
    "\n",
    "    def all_answers(self):\n",
    "        return self.anss.copy()\n",
    "\n",
    "    @property\n",
    "    def num_answers(self):\n",
    "        return len(self.anss)\n",
    "\n",
    "\n",
    "def load_lxmert_qa(path, model, label2ans):\n",
    "    \"\"\"\n",
    "    Load model weights from LXMERT pre-training.\n",
    "    The answers in the fine-tuned QA task (indicated by label2ans)\n",
    "        would also be properly initialized with LXMERT pre-trained\n",
    "        QA heads.\n",
    "\n",
    "    :param path: Path to LXMERT snapshot.\n",
    "    :param model: LXRT model instance.\n",
    "    :param label2ans: The label2ans dict of fine-tuned QA datasets, like\n",
    "        {0: 'cat', 1: 'dog', ...}\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Load QA pre-trained LXMERT from %s \" % path)\n",
    "    loaded_state_dict = torch.load(\"%s_LXRT.pth\" % path, map_location=lambda storage, loc: storage)\n",
    "    model_state_dict = model.state_dict()\n",
    "\n",
    "    # Handle Multi-GPU pre-training --> Single GPU fine-tuning\n",
    "    for key in list(loaded_state_dict.keys()):\n",
    "        loaded_state_dict[key.replace(\"module.\", '')] = loaded_state_dict.pop(key)\n",
    "\n",
    "    # Isolate bert model\n",
    "    bert_state_dict = {}\n",
    "    for key, value in loaded_state_dict.items():\n",
    "        if key.startswith('bert.'):\n",
    "            bert_state_dict[key] = value\n",
    "\n",
    "    # Isolate answer head\n",
    "    answer_state_dict = {}\n",
    "    for key, value in loaded_state_dict.items():\n",
    "        if key.startswith(\"answer_head.\"):\n",
    "            answer_state_dict[key.replace('answer_head.', '')] = value\n",
    "\n",
    "    # Do surgery on answer state dict\n",
    "    ans_weight = answer_state_dict['logit_fc.3.weight']\n",
    "    ans_bias = answer_state_dict['logit_fc.3.bias']\n",
    "    import copy\n",
    "    new_answer_weight = copy.deepcopy(model_state_dict['logit_fc.3.weight'])\n",
    "    new_answer_bias = copy.deepcopy(model_state_dict['logit_fc.3.bias'])\n",
    "    answer_table = AnswerTable()\n",
    "    loaded = 0\n",
    "    unload = 0\n",
    "    if type(label2ans) is list:\n",
    "        label2ans = {label: ans for label, ans in enumerate(label2ans)}\n",
    "    for label, ans in label2ans.items():\n",
    "        new_ans = answer_table.convert_ans(ans)\n",
    "        if answer_table.used(new_ans):\n",
    "            ans_id_9500 = answer_table.ans2id(new_ans)\n",
    "            new_answer_weight[label] = ans_weight[ans_id_9500]\n",
    "            new_answer_bias[label] = ans_bias[ans_id_9500]\n",
    "            loaded += 1\n",
    "        else:\n",
    "            new_answer_weight[label] = 0.\n",
    "            new_answer_bias[label] = 0.\n",
    "            unload += 1\n",
    "    print(\"Loaded %d answers from LXRTQA pre-training and %d not\" % (loaded, unload))\n",
    "    print()\n",
    "    answer_state_dict['logit_fc.3.weight'] = new_answer_weight\n",
    "    answer_state_dict['logit_fc.3.bias'] = new_answer_bias\n",
    "\n",
    "    # Load Bert Weights\n",
    "    bert_model_keys = set(model.lxrt_encoder.model.state_dict().keys())\n",
    "    bert_loaded_keys = set(bert_state_dict.keys())\n",
    "    assert len(bert_model_keys - bert_loaded_keys) == 0\n",
    "    model.lxrt_encoder.model.load_state_dict(bert_state_dict, strict=False)\n",
    "\n",
    "    # Load Answer Logic FC Weights\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    ans_loaded_keys = set(answer_state_dict.keys())\n",
    "    assert len(ans_loaded_keys - model_keys) == 0\n",
    "\n",
    "    model.load_state_dict(answer_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load QA pre-trained LXMERT from /home/jupyter/vcr/lxmert/snap/pretrained/model \n",
      "Loaded 3124 answers from LXRTQA pre-training and 5 not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_lxmert_qa(load_lxmert_qa_path, model, label2ans= dset_train_init.label2ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = torch.load(load_lxmert_qa_path+'_LXRT.pth')\n",
    "# k.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VQAModel(\n",
       "  (lxrt_encoder): LXRTEncoder(\n",
       "    (model): LXRTFeatureExtraction(\n",
       "      (bert): LXRTModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "          (token_type_embeddings): Embedding(2, 768, padding_idx=0)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): LXRTEncoder(\n",
       "          (visn_fc): VisualFeatEncoder(\n",
       "            (visn_fc): Linear(in_features=2048, out_features=768, bias=True)\n",
       "            (visn_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (box_fc): Linear(in_features=4, out_features=768, bias=True)\n",
       "            (box_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (layer): ModuleList(\n",
       "            (0): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (x_layers): ModuleList(\n",
       "            (0): LXRTXLayer(\n",
       "              (visual_attention): BertCrossattLayer(\n",
       "                (att): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (visn_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (lang_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (visn_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (visn_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): LXRTXLayer(\n",
       "              (visual_attention): BertCrossattLayer(\n",
       "                (att): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (visn_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (lang_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (visn_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (visn_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): LXRTXLayer(\n",
       "              (visual_attention): BertCrossattLayer(\n",
       "                (att): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (visn_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (lang_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (visn_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (visn_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): LXRTXLayer(\n",
       "              (visual_attention): BertCrossattLayer(\n",
       "                (att): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (visn_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (lang_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (visn_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (visn_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): LXRTXLayer(\n",
       "              (visual_attention): BertCrossattLayer(\n",
       "                (att): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (visn_self_att): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (lang_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (lang_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (visn_inter): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (visn_output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (r_layers): ModuleList(\n",
       "            (0): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): BertLayer(\n",
       "              (attention): BertSelfattLayer(\n",
       "                (self): BertAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertAttOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (logit_fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): GeLU()\n",
       "    (2): LayerNorm((1536,), eps=1e-12, elementwise_affine=True)\n",
       "    (3): Linear(in_features=1536, out_features=3129, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(),1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_wrapper = (lambda x: tqdm(x, total=len(loader))) if tqdm else (lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid = 0\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(epochs)):\n",
    "#     quesid2ans = {}\n",
    "    \n",
    "#     for i, (ques_id, feats, boxes, sent, target) in iter_wrapper(enumerate(loader)):\n",
    "#         model.train()\n",
    "#         optim.zero_grad()\n",
    "        \n",
    "#         feats, boxes, target = feats.cuda(), boxes.cuda(), target.cuda()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0\n",
    "ques_id, feats, boxes, sent, target = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128]),\n",
       " torch.Size([128, 36, 2048]),\n",
       " torch.Size([128, 36, 4]),\n",
       " 128,\n",
       " torch.Size([128, 3129]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_id.shape, feats.shape, boxes.shape, len(sent), target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats, boxes, target = feats.cuda(), boxes.cuda(), target.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = model(feats,boxes,sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3129])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert logit.dim()==target.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = bce_loss(logit,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss*logit.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9287.52677944882"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.utils.clip_grad_norm_(model.parameters(), 5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "score,label = logit.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "quesid2ans = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "    ans = dset_train_init.label2ans[l]\n",
    "    quesid2ans[qid.item()] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{85527011: 'yes',\n",
       " 539971002: 'no',\n",
       " 480268000: 'unknown',\n",
       " 575916019: 'no',\n",
       " 127074002: 'in grass',\n",
       " 54796004: 'tennis racket',\n",
       " 178606003: 'kites',\n",
       " 143554006: 'nothing',\n",
       " 138179001: 'toothbrush',\n",
       " 513765001: 'yes',\n",
       " 15029000: '2',\n",
       " 441264000: 'open',\n",
       " 133418000: 'yes',\n",
       " 488915003: 'no',\n",
       " 476005000: 'buses',\n",
       " 553776001: 'yellow',\n",
       " 451674001: 'kite',\n",
       " 69911001: 'nothing',\n",
       " 205101012: '1',\n",
       " 211743002: 'yes',\n",
       " 504297013: \"mcdonald's\",\n",
       " 39900004: 'no',\n",
       " 140636001: 'black',\n",
       " 486491004: 'no',\n",
       " 12818001: 'yes',\n",
       " 216636002: 'chocolate',\n",
       " 443818001: '1',\n",
       " 339336002: 'yes',\n",
       " 489907005: 'no',\n",
       " 454148002: 'no',\n",
       " 168706002: 'no',\n",
       " 223874008: '2',\n",
       " 122166003: 'right',\n",
       " 377845000: '1',\n",
       " 161877007: 'brown',\n",
       " 101180000: 'red',\n",
       " 85527013: 'yes',\n",
       " 338683002: 'black',\n",
       " 204525008: 'pink',\n",
       " 38828003: 'leather',\n",
       " 563337001: 'brown',\n",
       " 445008004: '5',\n",
       " 332775002: 'suitcase',\n",
       " 8498000: 'afternoon',\n",
       " 514396000: 'no',\n",
       " 23584001: 'orange',\n",
       " 331455008: 'yes',\n",
       " 163290001: 'no',\n",
       " 160580004: 'balcony',\n",
       " 483130008: 'flowers',\n",
       " 435896001: '20',\n",
       " 51540000: 'grass',\n",
       " 488915035: 'nothing',\n",
       " 331455000: 'no',\n",
       " 200250001: 'unknown',\n",
       " 39769001: '2',\n",
       " 235597021: 'black',\n",
       " 231617006: 'tracks',\n",
       " 310407004: 'standing',\n",
       " 493846011: 'black',\n",
       " 124004006: 'no',\n",
       " 357758030: 'harley',\n",
       " 97656005: 'graffiti',\n",
       " 78741005: 'yes',\n",
       " 551974019: 'white',\n",
       " 62790001: 'no',\n",
       " 373793004: 'yes',\n",
       " 179181002: 'no',\n",
       " 351810000: '1',\n",
       " 560880006: 'brown',\n",
       " 202913000: 'no',\n",
       " 540473002: 'picture',\n",
       " 133131004: 'graffiti',\n",
       " 472623007: 'rocky',\n",
       " 192699003: 'bikes',\n",
       " 195267011: 'helmet',\n",
       " 268350004: 'living room',\n",
       " 345980004: 'no',\n",
       " 277622005: '1',\n",
       " 466347000: '0',\n",
       " 474028004: 'soccer',\n",
       " 133104000: 'yellow',\n",
       " 457449002: 'red',\n",
       " 151781001: 'boy',\n",
       " 24053020: 'no',\n",
       " 130171002: 'yes',\n",
       " 241319002: 'green',\n",
       " 126659004: 'airport',\n",
       " 295303001: 'tray',\n",
       " 38479000: 'small',\n",
       " 292991004: 'no',\n",
       " 364299008: 'blue',\n",
       " 287190000: 'no',\n",
       " 287151003: 'yes',\n",
       " 396410001: '1',\n",
       " 578485002: 'wood',\n",
       " 411223000: 'no',\n",
       " 506401004: \"mcdonald's\",\n",
       " 220699001: 'no',\n",
       " 467197001: 'bag',\n",
       " 109827002: 'teddy bear',\n",
       " 184321003: 'blue',\n",
       " 281837001: '4',\n",
       " 186797000: 'unknown',\n",
       " 211243000: 'water',\n",
       " 180366001: 'food',\n",
       " 423519000: 'no',\n",
       " 551974028: 'remote',\n",
       " 283152003: 'yes',\n",
       " 281512019: 'yes',\n",
       " 377946005: 'no',\n",
       " 246672000: 'babies',\n",
       " 448359002: 'woman',\n",
       " 477477005: 'winter',\n",
       " 563381011: 'yes',\n",
       " 243980001: 'dirt',\n",
       " 135029011: '0',\n",
       " 313924001: 'tennis racket',\n",
       " 358750001: '2',\n",
       " 539971013: 'blanket',\n",
       " 299000005: 'yes',\n",
       " 77402003: 'white',\n",
       " 457882002: '1',\n",
       " 517128002: 'airport',\n",
       " 259497015: 'yes',\n",
       " 540681004: '2',\n",
       " 151657015: 'no',\n",
       " 79969004: 'no'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quesid2ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(85527011) 425\n"
     ]
    }
   ],
   "source": [
    "for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "    print(qid,l)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "quesid2ans = {}\n",
    "for qid, l in zip(ques_id, label.cpu().numpy()):\n",
    "    ans = dset_train_init.label2ans[l]\n",
    "    quesid2ans[qid.item()] = ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(79969004)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{85527011: 'yes',\n",
       " 539971002: 'no',\n",
       " 480268000: 'unknown',\n",
       " 575916019: 'no',\n",
       " 127074002: 'in grass',\n",
       " 54796004: 'tennis racket',\n",
       " 178606003: 'kites',\n",
       " 143554006: 'nothing',\n",
       " 138179001: 'toothbrush',\n",
       " 513765001: 'yes',\n",
       " 15029000: '2',\n",
       " 441264000: 'open',\n",
       " 133418000: 'yes',\n",
       " 488915003: 'no',\n",
       " 476005000: 'buses',\n",
       " 553776001: 'yellow',\n",
       " 451674001: 'kite',\n",
       " 69911001: 'nothing',\n",
       " 205101012: '1',\n",
       " 211743002: 'yes',\n",
       " 504297013: \"mcdonald's\",\n",
       " 39900004: 'no',\n",
       " 140636001: 'black',\n",
       " 486491004: 'no',\n",
       " 12818001: 'yes',\n",
       " 216636002: 'chocolate',\n",
       " 443818001: '1',\n",
       " 339336002: 'yes',\n",
       " 489907005: 'no',\n",
       " 454148002: 'no',\n",
       " 168706002: 'no',\n",
       " 223874008: '2',\n",
       " 122166003: 'right',\n",
       " 377845000: '1',\n",
       " 161877007: 'brown',\n",
       " 101180000: 'red',\n",
       " 85527013: 'yes',\n",
       " 338683002: 'black',\n",
       " 204525008: 'pink',\n",
       " 38828003: 'leather',\n",
       " 563337001: 'brown',\n",
       " 445008004: '5',\n",
       " 332775002: 'suitcase',\n",
       " 8498000: 'afternoon',\n",
       " 514396000: 'no',\n",
       " 23584001: 'orange',\n",
       " 331455008: 'yes',\n",
       " 163290001: 'no',\n",
       " 160580004: 'balcony',\n",
       " 483130008: 'flowers',\n",
       " 435896001: '20',\n",
       " 51540000: 'grass',\n",
       " 488915035: 'nothing',\n",
       " 331455000: 'no',\n",
       " 200250001: 'unknown',\n",
       " 39769001: '2',\n",
       " 235597021: 'black',\n",
       " 231617006: 'tracks',\n",
       " 310407004: 'standing',\n",
       " 493846011: 'black',\n",
       " 124004006: 'no',\n",
       " 357758030: 'harley',\n",
       " 97656005: 'graffiti',\n",
       " 78741005: 'yes',\n",
       " 551974019: 'white',\n",
       " 62790001: 'no',\n",
       " 373793004: 'yes',\n",
       " 179181002: 'no',\n",
       " 351810000: '1',\n",
       " 560880006: 'brown',\n",
       " 202913000: 'no',\n",
       " 540473002: 'picture',\n",
       " 133131004: 'graffiti',\n",
       " 472623007: 'rocky',\n",
       " 192699003: 'bikes',\n",
       " 195267011: 'helmet',\n",
       " 268350004: 'living room',\n",
       " 345980004: 'no',\n",
       " 277622005: '1',\n",
       " 466347000: '0',\n",
       " 474028004: 'soccer',\n",
       " 133104000: 'yellow',\n",
       " 457449002: 'red',\n",
       " 151781001: 'boy',\n",
       " 24053020: 'no',\n",
       " 130171002: 'yes',\n",
       " 241319002: 'green',\n",
       " 126659004: 'airport',\n",
       " 295303001: 'tray',\n",
       " 38479000: 'small',\n",
       " 292991004: 'no',\n",
       " 364299008: 'blue',\n",
       " 287190000: 'no',\n",
       " 287151003: 'yes',\n",
       " 396410001: '1',\n",
       " 578485002: 'wood',\n",
       " 411223000: 'no',\n",
       " 506401004: \"mcdonald's\",\n",
       " 220699001: 'no',\n",
       " 467197001: 'bag',\n",
       " 109827002: 'teddy bear',\n",
       " 184321003: 'blue',\n",
       " 281837001: '4',\n",
       " 186797000: 'unknown',\n",
       " 211243000: 'water',\n",
       " 180366001: 'food',\n",
       " 423519000: 'no',\n",
       " 551974028: 'remote',\n",
       " 283152003: 'yes',\n",
       " 281512019: 'yes',\n",
       " 377946005: 'no',\n",
       " 246672000: 'babies',\n",
       " 448359002: 'woman',\n",
       " 477477005: 'winter',\n",
       " 563381011: 'yes',\n",
       " 243980001: 'dirt',\n",
       " 135029011: '0',\n",
       " 313924001: 'tennis racket',\n",
       " 358750001: '2',\n",
       " 539971013: 'blanket',\n",
       " 299000005: 'yes',\n",
       " 77402003: 'white',\n",
       " 457882002: '1',\n",
       " 517128002: 'airport',\n",
       " 259497015: 'yes',\n",
       " 540681004: '2',\n",
       " 151657015: 'no',\n",
       " 79969004: 'no'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quesid2ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
